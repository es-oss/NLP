{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymT5hMaGcWrV"
   },
   "source": [
    "التأكد من تنصيب تنسرفلو , نمباي , ريكويستس , تقدم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_P43ppjlHy4e"
   },
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow==2.0.1 numpy requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXPcT7VPcWrr"
   },
   "source": [
    "تحميل المكتبات المطلوبة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "huCNBa-AIBUs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_gke2-zcWr5"
   },
   "source": [
    "قراءة الملف من الرابط الخاص به"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIBo0gh4IFUS",
    "outputId": "6c1fdbaa-2752-4d64-e120-58f2c705e428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167516"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "content = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
    "open(\"wonderland.txt\", \"w\", encoding=\"utf-8\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygRQW7CLcWsH"
   },
   "source": [
    "تحديد المعاملات , وقراءة الملف و تنظيف البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wNl5ZHPUII87"
   },
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "# dataset file path\n",
    "FILE_PATH = \"wonderland.txt\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "# read the data\n",
    "text = open(FILE_PATH, encoding=\"utf-8\").read()\n",
    "# remove caps, comment this code if you want uppercase characters as well\n",
    "text = text.lower()\n",
    "# remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_-tGpoxcWsW"
   },
   "source": [
    " تحديد الحروف و الارقام و عددها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej-SscMwILbm",
    "outputId": "132ef5cd-06f0-4681-b450-0439def4b3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: \n",
      " 0123456789abcdefghijklmnopqrstuvwxyz﻿\n",
      "Number of characters: 154861\n",
      "Number of unique characters: 39\n"
     ]
    }
   ],
   "source": [
    "# print some stats\n",
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text)))\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T35W7YelcWsj"
   },
   "source": [
    "عمل قاموس من الحروف للاندكس و العكس"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6EtXDd_sINa1"
   },
   "outputs": [],
   "source": [
    "# dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "# dictionary that converts integers to characters\n",
    "int2char = {i: c for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhlwHBRIcWs0"
   },
   "source": [
    "عمل ملفات ليتم الحفظ عليها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K8V6LzelIPTt"
   },
   "outputs": [],
   "source": [
    "# save these dictionaries for later generation\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R_93VoicWtC"
   },
   "source": [
    "تحويل كل النص الي ارقام"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_dH9ZMDzIQ5U"
   },
   "outputs": [],
   "source": [
    "# convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAEC_927cWtO"
   },
   "source": [
    "تحويلها الي تنسور"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_u-lnbxBI5Sj"
   },
   "outputs": [],
   "source": [
    "# construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45PZ0c8ucWta"
   },
   "source": [
    "رؤية بعض الحروف و ارقامها , مع اعتبار ان المسافة لها رقم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw5fbVfTI7-L",
    "outputId": "a613fce4-2bd8-48e6-e310-124f747aa894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 ﻿\n",
      "27 p\n",
      "29 r\n",
      "26 o\n",
      "21 j\n",
      "16 e\n",
      "14 c\n",
      "31 t\n"
     ]
    }
   ],
   "source": [
    "# print first 5 characters\n",
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9KUxvXpcWtn"
   },
   "source": [
    "تقطيع النص كله الي بيانات مقطعة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeOe7V3gI9MT",
    "outputId": "20c7fbec-711a-413b-b06c-9814a6ca8e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever  you may copy it give it away or\n",
      "\n",
      "reuse it under the terms of the project gutenberg license included\n",
      "with this ebook or online at wwwgutenbergorg\n",
      "\n",
      "\n",
      "title alices adventures in wonderland\n",
      "\n",
      "author lewis carroll\n",
      "\n",
      "posting date june 25 2008\n"
     ]
    }
   ],
   "source": [
    "# build sequences by batching\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEE9ZGWVcWtz"
   },
   "source": [
    "تجهيز البيانات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ri5Z4GCmI_CR"
   },
   "outputs": [],
   "source": [
    "def split_sample(sample):\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2):\n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "        # extend the dataset with these samples by concatenate() method\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "# prepare inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VreHeB5mJDK0"
   },
   "outputs": [],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # onehot encode the inputs and the targets\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm4It5_TcWuL"
   },
   "source": [
    "استعراض بيانات اول قطعتين"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkoAHekjJE7T",
    "outputId": "dfa9562d-69ae-4e78-b55a-8ceb58481221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "this ebook is for the use of a\n",
      "Target: n\n",
      "Input shape: (100, 39)\n",
      "Target shape: (39,)\n",
      "================================================== \n",
      "\n",
      "Input: project gutenbergs alices adventures in wonderland by lewis carroll\n",
      "\n",
      "this ebook is for the use of an\n",
      "Target: y\n",
      "Input shape: (100, 39)\n",
      "Target shape: (39,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 2 samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhXuaZxycWub"
   },
   "source": [
    "عمل خلط للبيانات و تقسيمها الي باشتات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rgWLRKLhJGfC"
   },
   "outputs": [],
   "source": [
    "# repeat, shuffle and batch the dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvxqhziScWuo"
   },
   "source": [
    "بناء الموديل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4YMvYwpyJKHr"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BORAKe6VcWu0"
   },
   "source": [
    "عمل فولدر للنتائج , ثم الكومبايل , ثم التدريب , ثم الحفظ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYgORUFeJLsc",
    "outputId": "d06e1dbf-bac1-42bc-fd87-1a82109daa30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/1209 [..............................] - ETA: 46s - loss: 3.6487WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0221s vs `on_train_batch_end` time: 0.0426s). Check your callbacks.\n",
      "1209/1209 [==============================] - 48s 40ms/step - loss: 2.2953\n"
     ]
    }
   ],
   "source": [
    "# make results folder if does not exist yet\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "# train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=1)\n",
    "# save the model\n",
    "model.save(f\"results/{BASENAME}-{sequence_length}.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJPeHZaVcWu-"
   },
   "source": [
    "_____\n",
    "\n",
    "\n",
    "فيما بعد , يمكن فتح الموديل و استيراد المكتبات , وتحديد اسم الملف لقراءته , وايضا عمل كلمة البداية "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O564yOs5JlH8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
    "import os\n",
    "\n",
    "sequence_length = 100\n",
    "# dataset file path\n",
    "FILE_PATH = \"wonderland.txt\"\n",
    "# FILE_PATH = \"data/python_code.py\"\n",
    "BASENAME = os.path.basename(FILE_PATH)\n",
    "seed = \"chapter xiii\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzIItceecWvJ"
   },
   "source": [
    "فتح الملفات المحفوظة التي تم التدريب عليها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4GHGMhvBKkcR"
   },
   "outputs": [],
   "source": [
    "# load vocab dictionaries\n",
    "char2int = pickle.load(open(f\"/content/{BASENAME}-char2int.pickle\", \"rb\"))\n",
    "int2char = pickle.load(open(f\"/content/{BASENAME}-int2char.pickle\", \"rb\"))\n",
    "vocab_size = len(char2int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD23AdntcWvT"
   },
   "source": [
    "بناء الموديل لاستخدامه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "n0XeGzsxKr_n"
   },
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, vocab_size), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edA6WMhlcWve"
   },
   "source": [
    "قراءة الملفات التي تم حفظها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M8dQkpj8KuCL"
   },
   "outputs": [],
   "source": [
    "# load the optimal weights\n",
    "model.load_weights(f\"/content/results/{BASENAME}-{sequence_length}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6jtry1KcWvs"
   },
   "source": [
    "عمل التوقع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVI7mpYoK-Py",
    "outputId": "071df73f-c771-4eb6-9a27-684617385aef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 400/400 [00:14<00:00, 28.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: chapter xiii\n",
      "Generated text:\n",
      "n and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the work and the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = seed\n",
    "n_chars = 400\n",
    "# generate 400 characters\n",
    "generated = \"\"\n",
    "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
    "    # make the input sequence\n",
    "    X = np.zeros((1, sequence_length, vocab_size))\n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (sequence_length - len(seed)) + t, char2int[char]] = 1\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "    # converting the integer to a character\n",
    "    next_char = int2char[next_index]\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(\"Seed:\", s)\n",
    "print(\"Generated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q25hcgohK_eB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5.9.1 Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
